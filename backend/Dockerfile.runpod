# Video Translate Backend - RunPod/NVIDIA GPU Dockerfile
# Optimized for NVIDIA CUDA GPUs with faster-whisper and PyAnnote
# NOTE: This file expects the build context to be the repository root.

# ============================================================================
# Base stage with CUDA runtime + cuDNN 9 (required by faster-whisper)
# ============================================================================
FROM nvidia/cuda:12.4.1-cudnn-runtime-ubuntu22.04 AS base

# Install uv (The Python Package Installer)
COPY --from=ghcr.io/astral-sh/uv:latest /uv /bin/uv
# Install Deno (JS runtime for yt-dlp YouTube nsig/signature decoding)
# Node.js is also installed as fallback via apt
COPY --from=denoland/deno:bin /deno /bin/deno

# Prevent interactive prompts during build
ENV DEBIAN_FRONTEND=noninteractive

# Install system dependencies (No cache to save disk space)
RUN apt-get update && apt-get install -y --no-install-recommends \
    python3.11 \
    python3.11-dev \
    python3.11-venv \
    python3-pip \
    ffmpeg \
    git \
    curl \
    libsndfile1 \
    nodejs \
    && rm -rf /var/lib/apt/lists/*

# Set Python 3.11 as default
RUN update-alternatives --install /usr/bin/python python /usr/bin/python3.11 1 \
    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.11 1 \
    && update-alternatives --install /usr/bin/pip pip /usr/bin/pip3 1

# Upgrade pip (via uv, no cache)
RUN uv pip install --system --no-cache --upgrade pip setuptools wheel

WORKDIR /app

# ============================================================================
# Dependencies stage - Install all Python packages
# ============================================================================
FROM base AS dependencies

# Install PyTorch with CUDA 12.1 support FIRST (No cache)
RUN uv pip install --system --no-cache \
    torch==2.2.0 \
    torchaudio==2.2.0 \
    --index-url https://download.pytorch.org/whl/cu121

# Install faster-whisper for fast transcription
RUN uv pip install --system --no-cache faster-whisper

# Install ONNX Runtime with GPU support
RUN uv pip install --system --no-cache onnxruntime-gpu

# Install RunPod SDK
RUN uv pip install --system --no-cache runpod

# Install pyannote.audio (Diarization)
RUN uv pip install --system --no-cache pyannote.audio

# Install yt-dlp from master branch for maximum YouTube stability
# Master branch gets fixes faster than nightly (which is daily builds)
RUN uv pip install --system --no-cache --force-reinstall \
    "yt-dlp[default] @ https://github.com/yt-dlp/yt-dlp/archive/master.tar.gz"

# Copy and install base requirements (yt-dlp in requirements.txt will be skipped as already installed)
COPY backend/requirements.txt .
RUN uv pip install --system --no-cache -r requirements.txt

# ============================================================================
# Production stage
# ============================================================================
FROM dependencies AS production

# Copy application code to /app/backend/ to match import structure
COPY backend/ /app/backend/

# Set PYTHONPATH so "from backend.*" imports work
ENV PYTHONPATH=/app

WORKDIR /app/backend

# Create cache directories
RUN mkdir -p /app/cache \
    /root/.cache/whisper \
    /root/.cache/huggingface \
    /root/.cache/torch

# Set environment variables
ENV PLATFORM=runpod
ENV WHISPER_BACKEND=faster-whisper
ENV DIARIZATION_BACKEND=pyannote
ENV ENABLE_WHISPER=true
ENV WHISPER_MODEL=base
ENV WHISPER_BEAM_SIZE=5
ENV ENABLE_AUDIO_NORMALIZATION=true
ENV MODEL_CACHE_DIR=/root/.cache/video-translate-models
ENV CUDA_VISIBLE_DEVICES=0
ENV PYTHONUNBUFFERED=1
ENV PYTHONDONTWRITEBYTECODE=1

# Flask settings
ENV FLASK_ENV=production
ENV LOG_LEVEL=INFO
ENV PORT=5001

EXPOSE 5001

# Health check (increase start-period as loading models takes time)
HEALTHCHECK --interval=30s --timeout=10s --start-period=120s --retries=3 \
    CMD curl -f http://localhost:${PORT:-5001}/health || exit 1

# Default command: Run Flask server with configurable PORT
# Note: Using sync workers instead of gevent for better GPU memory handling
CMD gunicorn \
    --worker-class gthread \
    --workers 1 \
    --threads 4 \
    --bind "0.0.0.0:${PORT:-5001}" \
    --timeout 600 \
    --keep-alive 65 \
    --access-logfile - \
    --error-logfile - \
    app:app

# ============================================================================
# Serverless stage - For RunPod Serverless
# ============================================================================
FROM dependencies AS serverless

# Copy application code to /app/backend/ to match import structure
COPY backend/ /app/backend/

# Set PYTHONPATH so "from backend.*" imports work
ENV PYTHONPATH=/app

WORKDIR /app/backend

# Create cache directories
RUN mkdir -p /app/cache \
    /root/.cache/whisper \
    /root/.cache/huggingface \
    /root/.cache/torch

# Set environment variables
ENV PLATFORM=runpod
ENV WHISPER_BACKEND=faster-whisper
ENV DIARIZATION_BACKEND=pyannote
ENV ENABLE_WHISPER=true
ENV WHISPER_MODEL=base
ENV WHISPER_BEAM_SIZE=5
ENV MODEL_CACHE_DIR=/root/.cache/video-translate-models
ENV CUDA_VISIBLE_DEVICES=0
ENV PYTHONUNBUFFERED=1
ENV PYTHONDONTWRITEBYTECODE=1

# Pre-download models to reduce cold start (optional, increases image size)
# Uncomment to include models in image:
# RUN python3 -c "from faster_whisper import WhisperModel; WhisperModel('base', device='cpu', compute_type='int8');"

# Entry point for RunPod Serverless
CMD ["python", "-u", "runpod_handler.py"]
