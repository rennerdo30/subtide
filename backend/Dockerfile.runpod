# Video Translate Backend - RunPod/NVIDIA GPU Dockerfile
# Optimized for NVIDIA CUDA GPUs with faster-whisper and NeMo
# NOTE: This file expects the build context to be the 'backend/' directory.

# ============================================================================
# Base stage with CUDA runtime
# ============================================================================
FROM nvidia/cuda:12.1.1-cudnn8-runtime-ubuntu22.04 AS base

# Install uv (The Python Package Installer)
COPY --from=ghcr.io/astral-sh/uv:latest /uv /bin/uv

# Prevent interactive prompts during build
ENV DEBIAN_FRONTEND=noninteractive

# Install system dependencies (No cache to save disk space)
RUN apt-get update && apt-get install -y --no-install-recommends \
    python3.11 \
    python3.11-dev \
    python3.11-venv \
    python3-pip \
    ffmpeg \
    git \
    curl \
    libsndfile1 \
    && rm -rf /var/lib/apt/lists/*

# Set Python 3.11 as default
RUN update-alternatives --install /usr/bin/python python /usr/bin/python3.11 1 \
    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.11 1 \
    && update-alternatives --install /usr/bin/pip pip /usr/bin/pip3 1

# Upgrade pip (via uv, no cache)
RUN uv pip install --system --no-cache --upgrade pip setuptools wheel

WORKDIR /app

# ============================================================================
# Dependencies stage - Install all Python packages
# ============================================================================
FROM base AS dependencies

# Install PyTorch with CUDA 12.1 support FIRST (No cache)
RUN uv pip install --system --no-cache \
    torch==2.2.0 \
    torchaudio==2.2.0 \
    --index-url https://download.pytorch.org/whl/cu121

# Install faster-whisper for fast transcription
RUN uv pip install --system --no-cache faster-whisper

# Install ONNX Runtime with GPU support
RUN uv pip install --system --no-cache onnxruntime-gpu

# Install RunPod SDK
RUN uv pip install --system --no-cache runpod

# Install pyannote.audio (Diarization)
RUN uv pip install --system --no-cache pyannote.audio

# Copy and install base requirements
COPY backend/requirements.txt .
RUN uv pip install --system --no-cache -r requirements.txt

# ============================================================================
# Production stage
# ============================================================================
FROM dependencies AS production

# Copy application code
COPY backend/ .

# Create cache directories
RUN mkdir -p /app/cache \
    /root/.cache/whisper \
    /root/.cache/huggingface \
    /root/.cache/torch

# Set environment variables
ENV PLATFORM=runpod
ENV WHISPER_BACKEND=faster-whisper
ENV DIARIZATION_BACKEND=pyannote
ENV ENABLE_WHISPER=true
ENV WHISPER_MODEL=base
ENV CUDA_VISIBLE_DEVICES=0
ENV PYTHONUNBUFFERED=1
ENV PYTHONDONTWRITEBYTECODE=1

# Flask settings
ENV FLASK_ENV=production
ENV LOG_LEVEL=INFO

EXPOSE 5001

# Health check (increase start-period as loading models takes time)
HEALTHCHECK --interval=30s --timeout=10s --start-period=120s --retries=3 \
    CMD curl -f http://localhost:5001/health || exit 1

# Default command: Run Flask server
# Note: Using sync workers instead of gevent for better GPU memory handling
CMD ["gunicorn", \
    "--worker-class", "sync", \
    "--workers", "1", \
    "--threads", "4", \
    "--bind", "0.0.0.0:5001", \
    "--timeout", "600", \
    "--keep-alive", "65", \
    "--access-logfile", "-", \
    "--error-logfile", "-", \
    "app:app"]

# ============================================================================
# Serverless stage - For RunPod Serverless
# ============================================================================
FROM dependencies AS serverless

# Copy application code
COPY backend/ .

# Create cache directories
RUN mkdir -p /app/cache \
    /root/.cache/whisper \
    /root/.cache/huggingface \
    /root/.cache/torch

# Set environment variables
ENV PLATFORM=runpod
ENV WHISPER_BACKEND=faster-whisper
ENV DIARIZATION_BACKEND=pyannote
ENV ENABLE_WHISPER=true
ENV WHISPER_MODEL=base
ENV CUDA_VISIBLE_DEVICES=0
ENV PYTHONUNBUFFERED=1
ENV PYTHONDONTWRITEBYTECODE=1

# Pre-download models to reduce cold start (optional, increases image size)
# Uncomment to include models in image:
# RUN python -c "from faster_whisper import WhisperModel; WhisperModel('base', device='cpu')"

# Entry point for RunPod Serverless
CMD ["python", "-u", "runpod_handler.py"]
