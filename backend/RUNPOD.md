# RunPod Deployment Guide

Deploy Subtide on RunPod.io for fast, cost-effective GPU acceleration.

## üöÄ Quick Summary

| Feature | Serverless Queue | Dedicated Pod |
|---------|------------------|---------------|
| **Best For** | Variable loads, Tier 3 (Batch) | Continuous use, **Tier 4 (Streaming)** |
| **Cost** | Pay-per-second, scales to zero | Flat hourly rate |
| **Setup** | Easy, managed | Full control, accessible IP |
| **Streaming** | Limited (JSON chunks) | **Full SSE Support (Tier 4)** |
| **URL Format** | `https://api.runpod.ai/v2/{id}` | `https://pod-id-5001.proxy.runpod.net` |

---

## üê≥ Docker Image Selection

**Choose the correct image based on your deployment type:**

| Deployment Type | Docker Image | Entrypoint | URL Format |
|-----------------|--------------|------------|------------|
| **Serverless Queue** | `subtide-runpod:latest` | `runpod_handler.py` | `api.runpod.ai/v2/{id}/runsync` |
| **Load Balancer** | `subtide-runpod-server:latest` | `gunicorn` (Flask) | `{id}.api.runpod.ai/api/process` |
| **Dedicated Pod** | `subtide-runpod-server:latest` | `gunicorn` (Flask) | `pod-5001.proxy.runpod.net/api/process` |

### Understanding the Modes

**Serverless Queue** (`subtide-runpod`):
- Uses RunPod's job queue system
- Requests go to `/runsync` endpoint (RunPod SDK handles routing)
- No persistent HTTP server; workers process jobs from queue
- Best for: Variable workloads, pay-per-second, long-running tasks

**Load Balancer** (`subtide-runpod-server`):
- Runs Flask/Gunicorn HTTP server directly
- RunPod routes requests to healthy workers via `/ping` health checks
- Direct HTTP access to your custom endpoints (`/api/process`, `/api/stream`)
- Best for: Low latency, real-time streaming (Tier 4), custom REST APIs
- **Requires**: `/ping` endpoint returning 200 (healthy) or 204 (initializing)
- **Environment Variables**: `PORT` (default: 5001), `PORT_HEALTH` (default: same as PORT)
- See: [RunPod Load Balancing Docs](https://docs.runpod.io/serverless/load-balancing/overview)

**Dedicated Pod** (`subtide-runpod-server`):
- Same image as Load Balancer, but always-on single instance
- Fixed hourly cost, no cold starts
- Full control over the container

### Image Tags

All images are tagged with:
- `latest` ‚Äî Latest build from main branch
- `{short-sha}` ‚Äî Specific commit (e.g., `3289d96`)
- `v{version}` ‚Äî Semantic version (when tagged)

---

## üîí Authentication

All RunPod endpoints require a valid RunPod API Key.
Your client (Extension or API consumer) must send the key in the `Authorization` header:

1. Go to [RunPod Settings](https://www.runpod.io/console/user/settings).
2. Scroll to **API Keys**.
3. Create a new **Read/Write** key.

```
Authorization: Bearer <YOUR_RUNPOD_API_KEY>
```

### Client / Extension Configuration
1. Open the **Subtide Extension Settings**.
2. Select **Tier 3 (Batch)** or **Tier 4 (Stream)**.
3. **Backend URL**:
   - **Serverless Queue**: `https://api.runpod.ai/v2/{ENDPOINT_ID}` (no trailing slash)
   - **Dedicated Pod**: `https://pod-id-5001.proxy.runpod.net`
4. **Backend API Key**: Paste your RunPod API Key (starts with `rpa_...`).
5. Click **Save** and then **Check Backend** to verify connectivity.

> **Important**: The extension auto-detects the endpoint type from the URL format and uses the correct API calls automatically.

---

## üõ† Deployment Options

### Option 1: Dedicated Pod (Recommended for Tier 4 Streaming)

This runs the full Flask backend, providing native SSE support for the "live streaming" subtitle experience (`/api/stream`).

#### 1. Deploy
1. Go to [RunPod Pods](https://www.runpod.io/console/pods).
2. Click **Deploy**.
3. Choose **RTX 4090** (Best value), **RTX 3090**, or **RTX A4500** (High Availability).
4. **Customize Deployment**:
   - **Container Image**: `ghcr.io/rennerdo30/subtide-runpod-server:latest`
   - **Expose Port**: `5001` (HTTP)
   - **Environment Variables**:
     ```env
     SERVER_API_KEY=sk-your-openai-key   # Required for translation
     ENABLE_WHISPER=true
     WHISPER_MODEL=base
     ENABLE_DIARIZATION=true
     HF_TOKEN=hf_your-token             # Required for PyAnnote Diarization
     ```

#### 2. Get Backend Address
1. Once running, click **Connect**.
2. Find the **HTTP Service** mapped to port `5001`.
3. It will look like: `https://pod-id-5001.proxy.runpod.net`
4. **Copy this URL** into the Extension settings.

---

### Option 2: Serverless Queue (Best for Tier 3 / Batch)

Best for on-demand usage where you don't want to pay for idle time.

#### 1. Create Template
1. Go to [Templates](https://www.runpod.io/console/serverless/user/templates).
2. Click **New Template**.
3. **Container Image**: `ghcr.io/rennerdo30/subtide-runpod:latest`
4. **Container Disk**: `20 GB`.
5. **Environment Variables**:
   ```env
   HF_TOKEN=hf_your-token              # Required for PyAnnote Diarization
   SERVER_API_KEY=sk-your-openai-key   # Optional: for included translation
   WHISPER_MODEL=base                  # Options: tiny, base, small, medium, large
   ```

#### 2. Create Endpoint
1. Go to [Serverless](https://www.runpod.io/console/serverless).
2. Click **New Endpoint**.
3. Select your template.
4. **Min Workers**: `0` (Scales to zero to save cost).
5. **Max Workers**: `3`.
6. **FlashBoot**: Enabled (Faster cold starts).

#### 3. Get Endpoint URL
1. Click on your new Endpoint.
2. Copy the **Endpoint ID** (e.g., `abc123xyz`).
3. Your **Endpoint URL** is: `https://api.runpod.ai/v2/{endpoint_id}`
   - Example: `https://api.runpod.ai/v2/abc123xyz`
   - **Do NOT include a trailing slash**

---

### Option 3: Connect GitHub Repo (RunPod Serverless Repos)

Build directly from your Git repository without managing Docker registries.

1. Go to [RunPod Serverless](https://www.runpod.io/console/serverless).
2. Click **New Endpoint**.
3. Select **Import Git Repository**.
4. Authorize GitHub (if needed) and select this repository.
5. **Configuration**:
   - **Dockerfile Path**: `backend/Dockerfile.runpod`
   - **Context Directory**: leave default (Root)
   - **Branch**: `main` (or your release branch)
6. RunPod will auto-build the image from the source code.

> **Note**: This uses `backend/Dockerfile.runpod` which is optimized to run from the repository root.

---



---

## üèó Building Your Own Image

If you want to modify the backend and deploy your own version:

```bash
# Login to GitHub Registry (or DockerHub)
docker login ghcr.io

# Build Serverless Image
docker build -f Dockerfile -t ghcr.io/yourname/vt-runpod .

# Build Dedicated Server Image
# (Same Dockerfile, different entry command via docker-compose or override)
# The default Dockerfile is hybrid, but we use targets for clarity in CI/CD.
```

The provided `Dockerfile` in the root is optimized for both use cases.

## üì¶ Troubleshooting

### 401/403 Permission Denied
- Ensure your **RunPod API Key** is entered in the Extension's **Backend API Key** field (not the LLM API Key field).
- Ensure the key has **Read/Write** permissions (not Read Only).
- Verify the Endpoint ID in your URL matches the one in your RunPod dashboard.
- For Serverless: Use `https://api.runpod.ai/v2/{id}` format, NOT `https://{id}.api.runpod.ai`.

### 404 Not Found
- Ensure there is **no trailing slash** in the Backend URL.
- Verify the endpoint is deployed and showing as **Active** in RunPod dashboard.
- For Serverless Queue, the extension automatically appends `/runsync` to your URL.

### "Connection Refused" (Dedicated Pod)
- Ensure the Pod is showing **Running** (not Idle or Stopped).
- Check that port `5001` is exposed in the Pod settings.
- View logs in the RunPod console to check for startup errors.

### ModuleNotFoundError: No module named 'backend'
- This indicates an outdated Docker image. Rebuild with the latest Dockerfile that includes `PYTHONPATH=/app`.
- Pull the latest image: `ghcr.io/rennerdo30/subtide-runpod:latest`

### Streaming Lag (Serverless)
- Serverless cold starts can take 10-30s depending on GPU availability.
- Use **FlashBoot** to reduce cold start times.
- Use **Dedicated Pods** for instant, latency-sensitive streaming.

## ‚ö†Ô∏è Known Limitations

### Caching on Serverless
Serverless workers don't have persistent storage by default:
- **Audio cache**: Downloaded audio is lost when worker shuts down
- **Translation cache**: Previous translations are not preserved
- **Model weights**: Re-downloaded on each cold start (use custom image with pre-loaded models)

**Environment Variables to Control Cache:**
```env
CACHE_DIR=/tmp/cache               # Custom cache directory path
CACHE_MAX_SIZE_MB=1000             # Max cache size in MB (default: 5000)
CACHE_AUDIO_TTL_HOURS=1            # Audio file TTL (default: 24)
CACHE_CLEANUP_INTERVAL_MINUTES=10  # Cleanup interval (default: 30)
```

**Recommended RunPod Settings:**
```env
CACHE_MAX_SIZE_MB=500              # Limit to 500MB for serverless
CACHE_AUDIO_TTL_HOURS=1            # Short TTL (1 hour)
```

**Other Solutions:**
1. **Network Volumes**: Attach persistent storage to your endpoint for cache persistence
2. **Accept Cache Miss**: For pay-per-use, re-downloading is acceptable trade-off
3. **Dedicated Pods**: Use for persistent cache requirements

### Load Balancer Timeouts & 502 Errors
RunPod Load Balancer has a strict **5.5 minute processing timeout**. If a response is not sent within this time, the connection is terminated (Cloudflare 502/524 error).

**Mitigation (Implemented):**
1.  **Startup Model Preloading**: Models are loaded when the container starts, preventing cold-start timeouts on the first request.
2.  **SSE Streaming**: The Extension now uses Server-Sent Events (streaming) for Load Balancer requests. This sends "heartbeats" to keep the connection alive indefinitely, bypassing the 5.5 minute limit even for long videos.

**Note**: Ensure your Extension is updated to the latest version to utilize SSE streaming.

## üìö Reference: GPU IDs & Pools

When configuring `gpuTypeId` in `.runpod/tests.json` or `runpod.toml`, use **Pool IDs** for better availability during validation/testing. Use specific **GPU IDs** if you strictly require a certain hardware tier.

### üèä GPU Pools (Recommended for Validation)
| Pool ID | Included GPUs | VRAM |
| :--- | :--- | :--- |
| `AMPERE_16` | A4000, A4500, RTX 4000, RTX 2000 | 16 GB |
| `AMPERE_24` | L4, A5000, 3090 | 24 GB |
| `ADA_24` | 4090 | 24 GB |
| `AMPERE_48` | A6000, A40 | 48 GB |
| `ADA_48_PRO` | L40, L40S, 6000 Ada | 48 GB |
| `AMPERE_80` | A100 | 80 GB |
| `ADA_80_PRO` | H100 | 80 GB |
| `HOPPER_141` | H200 | 141 GB |

### üÜî Specific GPU IDs
**NVIDIA GeForce (Consumer)**
| ID | Model | VRAM |
| :--- | :--- | :--- |
| `NVIDIA GeForce RTX 3070` | RTX 3070 | 8 GB |
| `NVIDIA GeForce RTX 3080` | RTX 3080 | 10 GB |
| `NVIDIA GeForce RTX 3080 Ti` | RTX 3080 Ti | 12 GB |
| `NVIDIA GeForce RTX 3090` | RTX 3090 | 24 GB |
| `NVIDIA GeForce RTX 3090 Ti` | RTX 3090 Ti | 24 GB |
| `NVIDIA GeForce RTX 4070 Ti` | RTX 4070 Ti | 12 GB |
| `NVIDIA GeForce RTX 4080` | RTX 4080 | 16 GB |
| `NVIDIA GeForce RTX 4080 SUPER` | RTX 4080 SUPER | 16 GB |
| `NVIDIA GeForce RTX 4090` | RTX 4090 | 24 GB |
| `NVIDIA GeForce RTX 5080` | RTX 5080 | 16 GB |
| `NVIDIA GeForce RTX 5090` | RTX 5090 | 32 GB |

**NVIDIA RTX / Quadro (Professional)**
| ID | Model | VRAM |
| :--- | :--- | :--- |
| `NVIDIA RTX A2000` | RTX A2000 | 6 GB |
| `NVIDIA RTX A4000` | RTX A4000 | 16 GB |
| `NVIDIA RTX A4500` | RTX A4500 | 20 GB |
| `NVIDIA RTX A5000` | RTX A5000 | 24 GB |
| `NVIDIA RTX A6000` | RTX A6000 | 48 GB |
| `NVIDIA RTX 2000 Ada Generation` | RTX 2000 Ada | 16 GB |
| `NVIDIA RTX 4000 Ada Generation` | RTX 4000 Ada | 20 GB |
| `NVIDIA RTX 4000 SFF Ada Generation` | RTX 4000 Ada SFF | 20 GB |
| `NVIDIA RTX 5000 Ada Generation` | RTX 5000 Ada | 32 GB |
| `NVIDIA RTX 6000 Ada Generation` | RTX 6000 Ada | 48 GB |
| `NVIDIA RTX PRO 6000 Blackwell Server Edition` | RTX PRO 6000 Server | 96 GB |
| `NVIDIA RTX PRO 6000 Blackwell Workstation Edition` | RTX PRO 6000 Workstation | 96 GB |

**NVIDIA Data Center**
| ID | Model | VRAM |
| :--- | :--- | :--- |
| `NVIDIA L4` | L4 | 24 GB |
| `NVIDIA L40` | L40 | 48 GB |
| `NVIDIA L40S` | L40S | 48 GB |
| `NVIDIA A30` | A30 | 24 GB |
| `NVIDIA A40` | A40 | 48 GB |
| `NVIDIA A100 80GB PCIe` | A100 PCIe | 80 GB |
| `NVIDIA A100-SXM4-80GB` | A100 SXM | 80 GB |
| `NVIDIA H100 PCIe` | H100 PCIe | 80 GB |
| `NVIDIA H100 80GB HBM3` | H100 SXM | 80 GB |
| `NVIDIA H100 NVL` | H100 NVL | 94 GB |
| `NVIDIA H200` | H200 SXM | 141 GB |
| `NVIDIA B200` | B200 | 180 GB |
| `Tesla V100-FHHL-16GB` | V100 FHHL | 16 GB |
| `Tesla V100-PCIE-16GB` | Tesla V100 | 16 GB |
| `Tesla V100-SXM2-16GB` | V100 SXM2 | 16 GB |
| `Tesla V100-SXM2-32GB` | V100 SXM2 32GB | 32 GB |

**AMD**
| ID | Model | VRAM |
| :--- | :--- | :--- |
| `AMD Instinct MI300X OAM` | MI300X | 192 GB |
