# Subtide Backend - Environment Configuration
# Copy this file to .env and configure as needed

# =============================================================================
# Server Configuration
# =============================================================================
HOST=0.0.0.0
PORT=5001

# =============================================================================
# Whisper Configuration (for Tier 2 and Tier 3)
# =============================================================================
# Enable Whisper transcription for videos without subtitles
ENABLE_WHISPER=true

# Whisper model size: tiny, base, small, medium, large
# Larger models are more accurate but require more memory and time
WHISPER_MODEL=base

# Beam size for Whisper decoding (higher = more accurate but slower)
# Recommended: 5 for accuracy, 1 for speed
WHISPER_BEAM_SIZE=5

# Audio normalization (boosts quiet voices for better transcription)
ENABLE_AUDIO_NORMALIZATION=true

# Force source language (e.g., 'ja' for Japanese, 'en' for English)
# Leave empty for auto-detection (recommended for unknown content)
# Set this if you know the source language to prevent detection errors
# WHISPER_LANGUAGE=ja

# Speaker Diarization (Requires HF_TOKEN)
ENABLE_DIARIZATION=true
HF_TOKEN=your_huggingface_token_here

# =============================================================================
# LLM Provider Configuration
# =============================================================================
# Select the active LLM provider:
# Options: openai, anthropic, google, mistral, ollama, openrouter, deepseek
LLM_PROVIDER=openai

# -----------------------------------------------------------------------------
# API Keys (Set the key for your active provider)
# -----------------------------------------------------------------------------
# OPENAI_API_KEY=sk-...
# ANTHROPIC_API_KEY=sk-ant-...
# GOOGLE_API_KEY=AIza...
# MISTRAL_API_KEY=...
# OPENROUTER_API_KEY=sk-or-...
# DEEPSEEK_API_KEY=sk-...

# -----------------------------------------------------------------------------
# Model Selection (Defaults shown)
# -----------------------------------------------------------------------------
# OPENAI_MODEL=gpt-5.2
# ANTHROPIC_MODEL=claude-3-5-sonnet-latest
# GOOGLE_MODEL=gemini-2.0-flash-exp
# MISTRAL_MODEL=mistral-large-latest
# OPENROUTER_MODEL=google/gemini-2.0-flash-exp:free
# DEEPSEEK_MODEL=deepseek-chat
# OLLAMA_MODEL=llama3.3

# -----------------------------------------------------------------------------
# Concurrency Configuration (Recommended defaults)
# -----------------------------------------------------------------------------
# Number of parallel requests to send to the provider.
# Higher values increase speed but may hit rate limits.
# OPENAI_CONCURRENT_REQUESTS=3
# ANTHROPIC_CONCURRENT_REQUESTS=2
# GOOGLE_CONCURRENT_REQUESTS=5
# MISTRAL_CONCURRENT_REQUESTS=2
# OPENROUTER_CONCURRENT_REQUESTS=5
# DEEPSEEK_CONCURRENT_REQUESTS=2
# OLLAMA_CONCURRENT_REQUESTS=1

# -----------------------------------------------------------------------------
# Advanced Configuration (Optional)
# -----------------------------------------------------------------------------
# For custom OpenAI-compatible endpoints (like local DeepSeek or LM Studio)
# Set LLM_PROVIDER=openai_compatible or LLM_PROVIDER=lmstudio
# SERVER_API_URL=http://localhost:1234/v1

# For remote Ollama instances
# OLLAMA_BASE_URL=http://localhost:11434

# Legacy config for backward compatibility
# SERVER_API_KEY=... (maps to OPENAI_API_KEY)

# -----------------------------------------------------------------------------
# Language Mapping (JSON)
# -----------------------------------------------------------------------------
# Use different models for different target languages
# Example: {"ja":"claude-3-haiku","ko":"claude-3-haiku","zh":"gemini-2.0-flash","default":"gpt-4o-mini"}
# MODEL_LANG_MAP={}

# =============================================================================
# Local LLM (LM Studio / Ollama)
# =============================================================================
# For fully local operation without cloud APIs

# LM Studio (starts on port 1234 by default):
# SERVER_API_URL=http://localhost:1234/v1
# SERVER_MODEL=llama3.1:8b
# SERVER_API_KEY=lm-studio

# Ollama (starts on port 11434 by default):
# SERVER_API_URL=http://localhost:11434/v1
# SERVER_MODEL=llama3.1:8b
# SERVER_API_KEY=ollama

# Recommended models for translation:
#   llama3.1:8b     - 4.7 GB, good quality, fast
#   mistral:7b      - 4.1 GB, good for European languages
#   qwen2.5:7b      - 4.4 GB, excellent for Asian languages
#   llama3.1:70b-q4 - 40 GB, best quality (requires 64GB+ RAM)


# =============================================================================
# Whisper Accuracy Tuning
# =============================================================================
# Lower no_speech_threshold = captures more speech (but may include noise)
# Higher = stricter filtering (may miss quiet speech)
# Default 0.4 is recommended - set to 0.3 for very quiet audio
WHISPER_NO_SPEECH_THRESHOLD=0.4
WHISPER_COMPRESSION_RATIO_THRESHOLD=2.4
WHISPER_LOGPROB_THRESHOLD=-1.0

# VAD (Voice Activity Detection) Post-Filter
# Disabled by default - Whisper backends have built-in speech detection
# Enable only if you're getting too many hallucinations in silent parts
# ENABLE_VAD=false

# =============================================================================
# Troubleshooting: No Speech Detected
# =============================================================================
# If Whisper detects no speech or very few segments:
# 1. Lower threshold: WHISPER_NO_SPEECH_THRESHOLD=0.3
# 2. Keep VAD disabled: ENABLE_VAD=false
# 3. Try audio normalization: ENABLE_AUDIO_NORMALIZATION=true
# 4. Use a larger model: WHISPER_MODEL=large-v3-turbo

# =============================================================================
# Platform Specific Overrides
# =============================================================================
# Explicitly set the platform (runpod, macos, linux-cuda, linux-cpu, windows)
# PLATFORM=runpod

# Override backends (useful for custom setups)
# WHISPER_BACKEND=faster-whisper
# DIARIZATION_BACKEND=nemo
